{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys \n",
    "import numpy as np  \n",
    "import collections\n",
    "import torch.utils.data as data \n",
    "import torch.nn as nn \n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import functional as F \n",
    "import math\n",
    "min_loss = 1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutiAtten(nn.Module):\n",
    "    \"\"\"多头注意力层, 当forward的参数只有x时输入x.shape = (batch_size, len, ndim) out.shape = (batch_size, len, ndim)\n",
    "    否则，输入k,q,v(batch_size, len, dim) --> out (batch_size, len, ndim)\"\"\"\n",
    "    def __init__(self, ndim, h, drop_v, masked_len=None, is_cross=False):\n",
    "        super(MutiAtten, self).__init__()\n",
    "        assert ndim % h == 0 \n",
    "        # x.shape = (batch_size, len, ndim)\n",
    "        if not is_cross:\n",
    "            self.x2kqv = nn.Linear(ndim, 3 * ndim)  # out.shape = (batch_size, len, ndim)\n",
    "        else:\n",
    "            self.wq = nn.Linear(ndim, ndim)\n",
    "            self.wv = nn.Linear(ndim, ndim)\n",
    "            self.wk = nn.Linear(ndim, ndim)\n",
    "\n",
    "        self.final_proj = nn.Linear(ndim, ndim)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_v)\n",
    "        # mask matrix(optional) #todo 初始化一个masked_len * masked_len 的掩码矩阵\n",
    "        if masked_len != None:\n",
    "            self.register_buffer(\"mask\", torch.tril(torch.ones(masked_len, masked_len)).view(1, 1, masked_len, masked_len))\n",
    "\n",
    "        self.h = h \n",
    "        self.ndim = ndim \n",
    "        self.masked_len = masked_len\n",
    "        self.atten = None\n",
    "\n",
    "    def forward(self, x, preinput):\n",
    "        batch_size, len, ndim = preinput.size() \n",
    "        # self attention:\n",
    "        if x == None:\n",
    "            k, q, v = self.x2kqv(preinput).split(self.ndim, dim=2) # shape = (batch_size, len, ndim)\n",
    "\n",
    "            # shape --> (batch_size, h, len, ndim // h)\n",
    "            for c in (k, q, v):\n",
    "                c = c.view(batch_size, len, self.h, -1).transpose(1, 2)\n",
    "            atten_weight = (q @ k.transpose(-2, -1)) * (1.0 / (k.size(-1)**(0.5)))\n",
    "            if self.masked_len != None:\n",
    "                atten_weight = atten_weight.masked_fill(self.mask[:,:,:len,:len] == 0, float('-inf'))\n",
    "            atten_weight = F.softmax(atten_weight, dim=-1)\n",
    "            atten_weight = self.dropout(atten_weight) # shape = (batch_size, h, len, len)\n",
    "\n",
    "            out = atten_weight @ v # shape = (batch_size, h, len, ndim // h)\n",
    "            self.attenn = atten_weight\n",
    "            # concat:\n",
    "            out = out.transpose(1, 2).contiguous().view(batch_size, len, ndim)\n",
    "\n",
    "            # project:\n",
    "            out = self.final_proj(out)\n",
    "            return out  \n",
    "        # todo cross attention:\n",
    "        else:\n",
    "            \n",
    "            q = self.wq(preinput).view(batch_size, -1, self.h, ndim // self.h).transpose(1, 2)\n",
    "            k = self.wk(x).view(batch_size, -1, self.h, ndim // self.h).transpose(1, 2)\n",
    "            v = self.wv(x).view(batch_size, -1, self.h, ndim // self.h).transpose(1, 2)\n",
    "            # shape = (batch_size, h, len, ndim // h)\n",
    "            atten_weight = (q @ k.transpose(-2, -1) * (1.0 / math.sqrt(k.size(-1)))) # (batch_size, h, lenq, lenk)\n",
    "            if self.masked_len != None:\n",
    "                atten_weight = atten_weight.masked_fill(self.mask[:,:,:len,:len] == 0, float('-inf'))\n",
    "            atten_weight = F.softmax(atten_weight, dim=-1)\n",
    "            atten_weight = self.dropout(atten_weight)\n",
    "            out = atten_weight @ v \n",
    "            self.atten = atten_weight \n",
    "            out = out.transpose(1, 2).contiguous().view(batch_size, len, ndim)\n",
    "            out = self.final_proj(out)\n",
    "            return out \n",
    "                    \n",
    "\n",
    "class fdfwd(nn.Module):\n",
    "    \"\"\"feedforward层\"\"\"\n",
    "    def __init__(self, ndim, ndim_hidden, drop_v=0.1):\n",
    "        super(fdfwd, self).__init__()\n",
    "        self.w1 = nn.Linear(ndim, ndim_hidden)\n",
    "        self.w2 = nn.Linear(ndim_hidden, ndim)\n",
    "        self.dropout = nn.Dropout(drop_v)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.w1(x).relu()\n",
    "        out = self.dropout(out)\n",
    "        out = self.w2(out)\n",
    "\n",
    "        return out  \n",
    "class wordEmbedding(nn.Module):\n",
    "    \"\"\"修改后的embedding层，就是将向量乘以sqrt(ndim)\"\"\"\n",
    "    def __init__(self, ndim, vocab):\n",
    "        super(wordEmbedding, self).__init__()\n",
    "        self.embd = nn.Embedding(vocab, ndim)\n",
    "        self.ndim = ndim \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embd(x) * math.sqrt(self.ndim)\n",
    "\n",
    "class postionalEncoding(nn.Module):\n",
    "    \"\"\"位置编码\"\"\"\n",
    "    def __init__(self, ndim, drop_v, maxlen=5000):\n",
    "        super(postionalEncoding, self).__init__()\n",
    "        PE = torch.zeros(maxlen, ndim)\n",
    "        pos = torch.arange(0, maxlen).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, ndim, 2) * -(math.log(10000.0) / ndim)) \n",
    "        PE[:, 0::2] = torch.sin(pos * div_term)\n",
    "        PE[:, 1::2] = torch.cos(pos * div_term)\n",
    "        PE = PE.unsqueeze(0)\n",
    "        self.register_buffer(\"PE\", PE)\n",
    "        self.dropout = nn.Dropout(drop_v)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.PE[:,:x.size(1)].requires_grad_(False))\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a = nn.Parameter(torch.ones(ndim))\n",
    "        self.eps = eps\n",
    "        self.b = nn.Parameter(torch.zeros(ndim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.a * (x - x.mean(-1, keepdim=True)) / (x.std(-1, keepdim=True) + self.eps) + self.b\n",
    "\n",
    "class encoder(nn.Module):\n",
    "    \"\"\"一个编码器块儿\"\"\"\n",
    "    def __init__(self, drop_v, ndim, h, ndim_hidden):\n",
    "        super(encoder, self).__init__()\n",
    "        self.attn = MutiAtten(ndim, h, drop_v)\n",
    "        self.feedforward = fdfwd(ndim, ndim_hidden, drop_v)\n",
    "\n",
    "        # self.layernorm0 = LayerNorm(ndim)\n",
    "        self.layernorm1 = LayerNorm(ndim)\n",
    "        self.layernorm2 = LayerNorm(ndim)\n",
    "        self.dropout = nn.Dropout(drop_v)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = self.layernorm0(x) #! 为了de一个bug，加上一个层归一化\n",
    "        x = x + self.dropout(self.attn(None, x))\n",
    "        x = self.layernorm1(x)\n",
    "        x = x + self.dropout(self.feedforward(x))\n",
    "        x = self.layernorm2(x)\n",
    "        return x \n",
    "\n",
    "class decoder(nn.Module):\n",
    "    \"\"\"一个解码器块\"\"\"\n",
    "    def __init__(self, ndim, h, drop_v, masked_len, ndim_hidden):\n",
    "        super(decoder, self).__init__()\n",
    "        self.selfattn = MutiAtten(ndim, h, drop_v, masked_len)\n",
    "        self.crossattn = MutiAtten(ndim, h, drop_v, None, True)\n",
    "        self.feedforward = fdfwd(ndim, ndim_hidden, drop_v)\n",
    "        self.dropout =nn.Dropout()\n",
    "        \n",
    "        # self.layernorm0 = LayerNorm(ndim) #!debug\n",
    "        self.layernorm1 = LayerNorm(ndim)\n",
    "        self.layernorm2 = LayerNorm(ndim)\n",
    "        self.layernorm3 = LayerNorm(ndim)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, preinput):\n",
    "        # x = self.layernorm0(x) #! 同上\n",
    "        preinput = preinput + self.dropout(self.selfattn(None, preinput))\n",
    "        preinput = self.layernorm1(preinput)\n",
    "        preinput = preinput + self.dropout(self.crossattn(x, preinput))\n",
    "        preinput = self.layernorm2(preinput)\n",
    "        preinput = preinput + self.dropout(self.feedforward(preinput))\n",
    "        preinput = self.layernorm3(preinput)\n",
    "        return preinput \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"最后的生成器部分\"\"\"\n",
    "    def __init__(self, ndim, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.lin = nn.Linear(ndim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.lin(x), dim=-1)\n",
    "\n",
    "\n",
    "class MYTransformer(nn.Module):\n",
    "    def __init__(self, ndim, vocab_src, vocab_tar, drop_v, h, ndim_hidden, num_layers, masked_len):\n",
    "        super(MYTransformer, self).__init__()\n",
    "        self.prepros_inputlayer = nn.Sequential(wordEmbedding(ndim, vocab_src),postionalEncoding(ndim, drop_v)) # (batch_size, len, vocab_size) --> (batch_size, len, ndim)\n",
    "        self.prepros_outlayer = nn.Sequential(wordEmbedding(ndim, vocab_tar),postionalEncoding(ndim, drop_v)) # (batch_size, len, vocab_size) --> (batch_size, len, ndim)\n",
    "        self.dropout = nn.Dropout(drop_v)\n",
    "        self.EncoderBlocks = nn.ModuleList([encoder(drop_v, ndim, h, ndim_hidden) for _ in range(num_layers)])\n",
    "        # self.layernorm = LayerNorm(ndim)\n",
    "        self.DecoderBlocks = nn.ModuleList([decoder(ndim, h, drop_v, masked_len, ndim_hidden) for _ in range(num_layers)])\n",
    "        self.dense = nn.Linear(ndim, vocab_tar)\n",
    "        self.generator = Generator(ndim, vocab_tar)\n",
    "    def encode(self, x):\n",
    "        \"\"\"接收的是尚未编码的向量\"\"\"\n",
    "        # x.shape = (batch_size, len, ndim)\n",
    "        for layer in self.prepros_inputlayer:\n",
    "            x = layer(x)\n",
    "        for layer in self.EncoderBlocks:\n",
    "            x = layer(x)\n",
    "        return x \n",
    "\n",
    "    def decode(self, x, preinput):\n",
    "        \"\"\"同上\"\"\"\n",
    "        for layer in self.prepros_outlayer:\n",
    "            preinput = layer(preinput)\n",
    "        for layer in self.DecoderBlocks:\n",
    "            preinput = layer(x, preinput)\n",
    "        return preinput \n",
    "    \n",
    "    def forward(self, x, preinput):\n",
    "        \"\"\"抛去generate的部分\"\"\"\n",
    "        out =  self.decode(self.encode(x), preinput)\n",
    "        return self.dense(out)\n",
    "\n",
    "    def generate(self, x):\n",
    "        \"\"\"生成最终的概率\"\"\"\n",
    "        return self.generator(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grad_clipping(net, theta):\n",
    "    \"\"\"Clip the gradient.\n",
    "\n",
    "    Defined in :numref:`sec_rnn_scratch`\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "        params = [p for p in params if p.grad is not None]\n",
    "    else:\n",
    "        params = net.params\n",
    "    # sum = 0\n",
    "    # for p in params:\n",
    "    #     if p.grad != None:\n",
    "    #         sum += p.grad ** 2\n",
    "    # norm = torch.sqrt(sum)\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm\n",
    "class Vocab:  #@save\n",
    "    \"\"\"文本词表\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # 按出现频率排序\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # 未知词元的索引为0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # 未知词元的索引为0\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "\n",
    "def count_corpus(tokens):  #@save\n",
    "    \"\"\"统计词元的频率\"\"\"\n",
    "    # 这里的tokens是1D列表或2D列表\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # 将词元列表展平成一个列表\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "def read_data(data_path):\n",
    "    \"\"\"载入“英语－法语”数据集\"\"\"\n",
    "    with open(os.path.join(data_path), 'r',\n",
    "             encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "def preprocess_nmt(text):\n",
    "    \"\"\"预处理“英语－法语”数据集\"\"\"\n",
    "    def no_space(char, prev_char):\n",
    "        return char in set(',.!?') and prev_char != ' '\n",
    "\n",
    "    # 使用空格替换不间断空格\n",
    "    # 使用小写字母替换大写字母\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
    "    # 在单词和标点符号之间插入空格\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "           for i, char in enumerate(text)]\n",
    "    return ''.join(out)\n",
    "\n",
    "# # print(text[:80])\n",
    "# #@save\n",
    "def tokenize_nmt(text, num_examples=None):\n",
    "    \"\"\"词元化“英语－法语”数据数据集\"\"\"\n",
    "    source, target = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if num_examples and i > num_examples:\n",
    "            break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            source.append(parts[0].split(' '))\n",
    "            target.append(parts[1].split(' '))\n",
    "    return source, target\n",
    "\n",
    "\n",
    "# len(src_vocab)\n",
    "# #@save\n",
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    \"\"\"截断或填充文本序列\"\"\"\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]  # 截断\n",
    "    return line + [padding_token] * (num_steps - len(line))  # 填充\n",
    "\n",
    "# #@save\n",
    "def build_array_nmt(lines, vocab, num_steps):\n",
    "    \"\"\"将机器翻译的文本序列转换成小批量\"\"\"\n",
    "    lines = [vocab[l] for l in lines]\n",
    "    lines = [l + [vocab['<eos>']] for l in lines]\n",
    "    array = torch.tensor([truncate_pad(\n",
    "        l, num_steps, vocab['<pad>']) for l in lines])\n",
    "    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
    "    return array, valid_len\n",
    "# #@save\n",
    "def load_data(batch_size, num_steps, data_path, num_examples=600):\n",
    "    \"\"\"返回翻译数据集的迭代器和词表\"\"\"\n",
    "    text = preprocess_nmt(read_data(data_path))\n",
    "    source, target = tokenize_nmt(text, num_examples)\n",
    "    src_vocab = Vocab(source, min_freq=2,\n",
    "                          reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    tgt_vocab = Vocab(target, min_freq=2,\n",
    "                          reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\n",
    "    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\n",
    "    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "    # import d2l \n",
    "    # data_iter = d2l.load_array(data_arrays, batch_size)\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    data_iter = data.DataLoader(dataset, batch_size, shuffle=True)\n",
    "    return data_iter, src_vocab, tgt_vocab\n",
    "\n",
    "\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"在序列中屏蔽不相关的项\"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                        device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "#@save\n",
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"带遮蔽的softmax交叉熵损失函数\"\"\"\n",
    "    # pred的形状：(batch_size,num_steps,vocab_size)\n",
    "    # label的形状：(batch_size,num_steps)\n",
    "    # valid_len的形状：(batch_size,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction='none'\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n",
    "            pred.permute(0, 2, 1), label)\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss\n",
    "\n",
    "#@save\n",
    "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device, from_scrach=True):\n",
    "    global min_loss\n",
    "    \"\"\"训练序列到序列模型\"\"\"\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "    if from_scrach:\n",
    "        net.apply(xavier_init_weights)\n",
    "    \n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    net.train()\n",
    "    epoches = [epoch+1 for epoch in range(num_epochs)]\n",
    "    loss_per_epoch = []\n",
    "    # animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "    #                  xlim=[10, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        # timer = d2l.Timer()\n",
    "        time_start = time.time()\n",
    "        # metric = d2l.Accumulator(2)  # 训练损失总和，词元数量\n",
    "        Loss_ntoken = [0, 0]\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\n",
    "                          device=device).reshape(-1, 1)\n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # 强制教学\n",
    "            # Y_hat, _ = net(X, dec_input, X_valid_len)\n",
    "            Y_hat = net(X, dec_input)\n",
    "            l = loss(Y_hat, Y, Y_valid_len)\n",
    "            l.sum().backward()      # 损失函数的标量进行“反向传播”\n",
    "            grad_clipping(net, 1)\n",
    "\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                # metric.add(l.sum(), num_tokens)\n",
    "                Loss_ntoken[0] += l.sum() \n",
    "                Loss_ntoken[1] += num_tokens\n",
    "            # print(f\"training on {str(device)}\")\n",
    "        loss_per_epoch.append(Loss_ntoken[0] / Loss_ntoken[1])\n",
    "        if epoch % 10 == 1 and min_loss > loss_per_epoch[-1]:\n",
    "            min_loss = loss_per_epoch[-1]\n",
    "            torch.save(net, 'model.pth')\n",
    "        # if (epoch + 1) % 1 == 0:\n",
    "            # animator.add(epoch + 1, (metric[0] / metric[1],))\n",
    "        print(f'epoch {epoch} loss {Loss_ntoken[0] / Loss_ntoken[1]:.3f}, {Loss_ntoken[1] / (time.time()-time_start):.1f} '\n",
    "            f'tokens/sec on {str(device)}')\n",
    "        \n",
    "    return epoches, loss_per_epoch\n",
    "#@save\n",
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,\n",
    "                    device, save_attention_weights=False):\n",
    "    \"\"\"序列到序列模型的预测\"\"\"\n",
    "    # 在预测时将net设置为评估模式\n",
    "    net.eval()\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [\n",
    "        src_vocab['<eos>']]\n",
    "    # enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
    "    src_tokens = truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    # 添加批量轴\n",
    "    enc_X = torch.unsqueeze(\n",
    "        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    enc_outputs = net.encode(enc_X)\n",
    "    # dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    # 添加批量轴\n",
    "    dec_X = torch.unsqueeze(torch.tensor(\n",
    "        [tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
    "    output_seq = []\n",
    "    for _ in range(num_steps):\n",
    "        Y = net.decode(enc_outputs, dec_X)\n",
    "        # Y = net.generate(Y)\n",
    "        # 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入\n",
    "        dec_X = Y.argmax(dim=-1)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # # 保存注意力权重（稍后讨论）\n",
    "        # if save_attention_weights:\n",
    "        #     attention_weight_seq.append(net.decoder.attention_weights)\n",
    "        # 一旦序列结束词元被预测，输出序列的生成就完成了\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    return ' '.join(tgt_vocab.to_tokens(output_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, src_vocab, tgt_vocab = load_data(batch_size=2, num_steps=10, data_path=r'data\\fra-eng\\fra.txt')\n",
    "ndim = 32\n",
    "vocab_src = len(src_vocab)\n",
    "vocab_tar = len(tgt_vocab)\n",
    "drop_v = 0.1 \n",
    "h =4\n",
    "ndim_hidden = 32 \n",
    "num_layers = 2 \n",
    "masked_len = max(vocab_src, vocab_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.368, 427.0 tokens/sec on cpu\n",
      "epoch 1 loss 0.300, 506.7 tokens/sec on cpu\n",
      "epoch 2 loss 0.268, 506.4 tokens/sec on cpu\n",
      "epoch 3 loss 0.250, 297.0 tokens/sec on cpu\n",
      "epoch 4 loss 0.239, 380.7 tokens/sec on cpu\n",
      "epoch 5 loss 0.226, 446.5 tokens/sec on cpu\n",
      "epoch 6 loss 0.220, 422.1 tokens/sec on cpu\n",
      "epoch 7 loss 0.212, 435.1 tokens/sec on cpu\n",
      "epoch 8 loss 0.206, 389.3 tokens/sec on cpu\n",
      "epoch 9 loss 0.200, 260.7 tokens/sec on cpu\n",
      "epoch 10 loss 0.193, 250.7 tokens/sec on cpu\n",
      "epoch 11 loss 0.192, 259.7 tokens/sec on cpu\n",
      "epoch 12 loss 0.185, 236.4 tokens/sec on cpu\n",
      "epoch 13 loss 0.179, 214.9 tokens/sec on cpu\n",
      "epoch 14 loss 0.175, 247.3 tokens/sec on cpu\n",
      "epoch 15 loss 0.169, 372.2 tokens/sec on cpu\n",
      "epoch 16 loss 0.164, 326.1 tokens/sec on cpu\n",
      "epoch 17 loss 0.160, 471.4 tokens/sec on cpu\n",
      "epoch 18 loss 0.156, 492.4 tokens/sec on cpu\n",
      "epoch 19 loss 0.151, 424.8 tokens/sec on cpu\n",
      "epoch 20 loss 0.147, 350.3 tokens/sec on cpu\n",
      "epoch 21 loss 0.144, 410.4 tokens/sec on cpu\n",
      "epoch 22 loss 0.141, 503.5 tokens/sec on cpu\n",
      "epoch 23 loss 0.137, 534.4 tokens/sec on cpu\n",
      "epoch 24 loss 0.131, 521.1 tokens/sec on cpu\n",
      "epoch 25 loss 0.131, 525.8 tokens/sec on cpu\n",
      "epoch 26 loss 0.128, 551.8 tokens/sec on cpu\n",
      "epoch 27 loss 0.124, 536.9 tokens/sec on cpu\n",
      "epoch 28 loss 0.120, 519.0 tokens/sec on cpu\n",
      "epoch 29 loss 0.119, 538.0 tokens/sec on cpu\n",
      "epoch 30 loss 0.115, 513.5 tokens/sec on cpu\n",
      "epoch 31 loss 0.112, 495.3 tokens/sec on cpu\n",
      "epoch 32 loss 0.113, 544.5 tokens/sec on cpu\n",
      "epoch 33 loss 0.108, 529.9 tokens/sec on cpu\n",
      "epoch 34 loss 0.108, 496.2 tokens/sec on cpu\n",
      "epoch 35 loss 0.106, 563.9 tokens/sec on cpu\n",
      "epoch 36 loss 0.103, 546.6 tokens/sec on cpu\n",
      "epoch 37 loss 0.104, 529.9 tokens/sec on cpu\n",
      "epoch 38 loss 0.104, 523.8 tokens/sec on cpu\n",
      "epoch 39 loss 0.101, 500.1 tokens/sec on cpu\n",
      "epoch 40 loss 0.099, 508.6 tokens/sec on cpu\n",
      "epoch 41 loss 0.097, 491.7 tokens/sec on cpu\n",
      "epoch 42 loss 0.096, 493.8 tokens/sec on cpu\n",
      "epoch 43 loss 0.096, 491.7 tokens/sec on cpu\n",
      "epoch 44 loss 0.093, 500.0 tokens/sec on cpu\n",
      "epoch 45 loss 0.091, 546.8 tokens/sec on cpu\n",
      "epoch 46 loss 0.091, 566.8 tokens/sec on cpu\n",
      "epoch 47 loss 0.089, 576.1 tokens/sec on cpu\n",
      "epoch 48 loss 0.091, 512.2 tokens/sec on cpu\n",
      "epoch 49 loss 0.090, 486.2 tokens/sec on cpu\n",
      "epoch 50 loss 0.086, 513.1 tokens/sec on cpu\n",
      "epoch 51 loss 0.085, 534.5 tokens/sec on cpu\n",
      "epoch 52 loss 0.086, 516.1 tokens/sec on cpu\n",
      "epoch 53 loss 0.086, 550.7 tokens/sec on cpu\n",
      "epoch 54 loss 0.083, 571.1 tokens/sec on cpu\n",
      "epoch 55 loss 0.083, 534.7 tokens/sec on cpu\n",
      "epoch 56 loss 0.081, 501.8 tokens/sec on cpu\n",
      "epoch 57 loss 0.081, 520.1 tokens/sec on cpu\n",
      "epoch 58 loss 0.082, 489.8 tokens/sec on cpu\n",
      "epoch 59 loss 0.082, 522.8 tokens/sec on cpu\n",
      "epoch 60 loss 0.078, 405.4 tokens/sec on cpu\n",
      "epoch 61 loss 0.079, 518.5 tokens/sec on cpu\n",
      "epoch 62 loss 0.078, 461.7 tokens/sec on cpu\n",
      "epoch 63 loss 0.081, 523.7 tokens/sec on cpu\n",
      "epoch 64 loss 0.076, 494.7 tokens/sec on cpu\n",
      "epoch 65 loss 0.077, 481.4 tokens/sec on cpu\n",
      "epoch 66 loss 0.079, 422.9 tokens/sec on cpu\n",
      "epoch 67 loss 0.072, 444.9 tokens/sec on cpu\n",
      "epoch 68 loss 0.075, 450.2 tokens/sec on cpu\n",
      "epoch 69 loss 0.073, 474.9 tokens/sec on cpu\n",
      "epoch 70 loss 0.072, 492.8 tokens/sec on cpu\n",
      "epoch 71 loss 0.072, 481.5 tokens/sec on cpu\n",
      "epoch 72 loss 0.073, 443.2 tokens/sec on cpu\n",
      "epoch 73 loss 0.071, 307.5 tokens/sec on cpu\n",
      "epoch 74 loss 0.071, 378.3 tokens/sec on cpu\n",
      "epoch 75 loss 0.069, 357.5 tokens/sec on cpu\n",
      "epoch 76 loss 0.069, 354.7 tokens/sec on cpu\n",
      "epoch 77 loss 0.069, 378.5 tokens/sec on cpu\n",
      "epoch 78 loss 0.068, 322.7 tokens/sec on cpu\n",
      "epoch 79 loss 0.070, 360.4 tokens/sec on cpu\n",
      "epoch 80 loss 0.068, 365.5 tokens/sec on cpu\n",
      "epoch 81 loss 0.068, 376.5 tokens/sec on cpu\n",
      "epoch 82 loss 0.068, 360.6 tokens/sec on cpu\n",
      "epoch 83 loss 0.066, 327.9 tokens/sec on cpu\n",
      "epoch 84 loss 0.066, 341.5 tokens/sec on cpu\n",
      "epoch 85 loss 0.067, 362.8 tokens/sec on cpu\n",
      "epoch 86 loss 0.065, 414.8 tokens/sec on cpu\n",
      "epoch 87 loss 0.064, 379.5 tokens/sec on cpu\n",
      "epoch 88 loss 0.064, 354.8 tokens/sec on cpu\n",
      "epoch 89 loss 0.064, 327.4 tokens/sec on cpu\n",
      "epoch 90 loss 0.064, 334.3 tokens/sec on cpu\n",
      "epoch 91 loss 0.066, 353.9 tokens/sec on cpu\n",
      "epoch 92 loss 0.063, 377.2 tokens/sec on cpu\n",
      "epoch 93 loss 0.064, 371.6 tokens/sec on cpu\n",
      "epoch 94 loss 0.061, 358.0 tokens/sec on cpu\n",
      "epoch 95 loss 0.061, 338.3 tokens/sec on cpu\n",
      "epoch 96 loss 0.062, 387.0 tokens/sec on cpu\n",
      "epoch 97 loss 0.062, 392.9 tokens/sec on cpu\n",
      "epoch 98 loss 0.062, 362.7 tokens/sec on cpu\n",
      "epoch 99 loss 0.061, 251.3 tokens/sec on cpu\n",
      "epoch 100 loss 0.062, 192.3 tokens/sec on cpu\n",
      "epoch 101 loss 0.059, 264.0 tokens/sec on cpu\n",
      "epoch 102 loss 0.059, 336.4 tokens/sec on cpu\n",
      "epoch 103 loss 0.061, 330.2 tokens/sec on cpu\n",
      "epoch 104 loss 0.060, 326.9 tokens/sec on cpu\n",
      "epoch 105 loss 0.060, 321.5 tokens/sec on cpu\n",
      "epoch 106 loss 0.058, 372.1 tokens/sec on cpu\n",
      "epoch 107 loss 0.059, 324.9 tokens/sec on cpu\n",
      "epoch 108 loss 0.059, 370.9 tokens/sec on cpu\n",
      "epoch 109 loss 0.059, 226.2 tokens/sec on cpu\n",
      "epoch 110 loss 0.058, 347.7 tokens/sec on cpu\n",
      "epoch 111 loss 0.058, 298.0 tokens/sec on cpu\n",
      "epoch 112 loss 0.060, 386.4 tokens/sec on cpu\n",
      "epoch 113 loss 0.057, 328.7 tokens/sec on cpu\n",
      "epoch 114 loss 0.058, 364.5 tokens/sec on cpu\n",
      "epoch 115 loss 0.057, 331.1 tokens/sec on cpu\n",
      "epoch 116 loss 0.057, 340.7 tokens/sec on cpu\n",
      "epoch 117 loss 0.058, 402.6 tokens/sec on cpu\n",
      "epoch 118 loss 0.056, 324.4 tokens/sec on cpu\n",
      "epoch 119 loss 0.057, 357.0 tokens/sec on cpu\n",
      "epoch 120 loss 0.057, 307.6 tokens/sec on cpu\n",
      "epoch 121 loss 0.056, 315.5 tokens/sec on cpu\n",
      "epoch 122 loss 0.055, 349.5 tokens/sec on cpu\n",
      "epoch 123 loss 0.055, 356.8 tokens/sec on cpu\n",
      "epoch 124 loss 0.056, 360.8 tokens/sec on cpu\n",
      "epoch 125 loss 0.057, 343.8 tokens/sec on cpu\n",
      "epoch 126 loss 0.055, 346.3 tokens/sec on cpu\n",
      "epoch 127 loss 0.055, 346.8 tokens/sec on cpu\n",
      "epoch 128 loss 0.055, 305.6 tokens/sec on cpu\n",
      "epoch 129 loss 0.056, 313.3 tokens/sec on cpu\n",
      "epoch 130 loss 0.056, 413.4 tokens/sec on cpu\n",
      "epoch 131 loss 0.054, 425.2 tokens/sec on cpu\n",
      "epoch 132 loss 0.056, 534.4 tokens/sec on cpu\n",
      "epoch 133 loss 0.052, 497.5 tokens/sec on cpu\n",
      "epoch 134 loss 0.055, 484.9 tokens/sec on cpu\n",
      "epoch 135 loss 0.054, 391.9 tokens/sec on cpu\n",
      "epoch 136 loss 0.052, 441.4 tokens/sec on cpu\n",
      "epoch 137 loss 0.055, 444.1 tokens/sec on cpu\n",
      "epoch 138 loss 0.057, 476.1 tokens/sec on cpu\n",
      "epoch 139 loss 0.054, 456.7 tokens/sec on cpu\n",
      "epoch 140 loss 0.051, 491.7 tokens/sec on cpu\n",
      "epoch 141 loss 0.053, 496.7 tokens/sec on cpu\n",
      "epoch 142 loss 0.051, 548.3 tokens/sec on cpu\n",
      "epoch 143 loss 0.054, 481.2 tokens/sec on cpu\n",
      "epoch 144 loss 0.054, 474.4 tokens/sec on cpu\n",
      "epoch 145 loss 0.052, 459.4 tokens/sec on cpu\n",
      "epoch 146 loss 0.053, 476.0 tokens/sec on cpu\n",
      "epoch 147 loss 0.051, 475.2 tokens/sec on cpu\n",
      "epoch 148 loss 0.051, 493.5 tokens/sec on cpu\n",
      "epoch 149 loss 0.051, 483.1 tokens/sec on cpu\n",
      "epoch 150 loss 0.051, 533.5 tokens/sec on cpu\n",
      "epoch 151 loss 0.052, 468.7 tokens/sec on cpu\n",
      "epoch 152 loss 0.054, 467.8 tokens/sec on cpu\n",
      "epoch 153 loss 0.050, 496.7 tokens/sec on cpu\n",
      "epoch 154 loss 0.049, 517.1 tokens/sec on cpu\n",
      "epoch 155 loss 0.050, 492.1 tokens/sec on cpu\n",
      "epoch 156 loss 0.053, 443.9 tokens/sec on cpu\n",
      "epoch 157 loss 0.052, 389.1 tokens/sec on cpu\n",
      "epoch 158 loss 0.052, 507.3 tokens/sec on cpu\n",
      "epoch 159 loss 0.050, 469.3 tokens/sec on cpu\n",
      "epoch 160 loss 0.052, 486.7 tokens/sec on cpu\n",
      "epoch 161 loss 0.050, 463.6 tokens/sec on cpu\n",
      "epoch 162 loss 0.050, 450.6 tokens/sec on cpu\n",
      "epoch 163 loss 0.050, 455.4 tokens/sec on cpu\n",
      "epoch 164 loss 0.049, 515.6 tokens/sec on cpu\n",
      "epoch 165 loss 0.050, 469.8 tokens/sec on cpu\n",
      "epoch 166 loss 0.049, 457.8 tokens/sec on cpu\n",
      "epoch 167 loss 0.050, 446.7 tokens/sec on cpu\n",
      "epoch 168 loss 0.053, 475.8 tokens/sec on cpu\n",
      "epoch 169 loss 0.050, 460.3 tokens/sec on cpu\n",
      "epoch 170 loss 0.049, 482.7 tokens/sec on cpu\n",
      "epoch 171 loss 0.049, 474.3 tokens/sec on cpu\n",
      "epoch 172 loss 0.051, 534.6 tokens/sec on cpu\n",
      "epoch 173 loss 0.050, 515.3 tokens/sec on cpu\n",
      "epoch 174 loss 0.048, 494.5 tokens/sec on cpu\n",
      "epoch 175 loss 0.050, 326.5 tokens/sec on cpu\n",
      "epoch 176 loss 0.048, 378.8 tokens/sec on cpu\n",
      "epoch 177 loss 0.050, 548.9 tokens/sec on cpu\n",
      "epoch 178 loss 0.048, 522.6 tokens/sec on cpu\n",
      "epoch 179 loss 0.049, 473.8 tokens/sec on cpu\n",
      "epoch 180 loss 0.049, 476.2 tokens/sec on cpu\n",
      "epoch 181 loss 0.050, 437.8 tokens/sec on cpu\n",
      "epoch 182 loss 0.049, 480.3 tokens/sec on cpu\n",
      "epoch 183 loss 0.050, 525.0 tokens/sec on cpu\n",
      "epoch 184 loss 0.049, 456.2 tokens/sec on cpu\n",
      "epoch 185 loss 0.047, 494.7 tokens/sec on cpu\n",
      "epoch 186 loss 0.050, 502.3 tokens/sec on cpu\n",
      "epoch 187 loss 0.048, 493.1 tokens/sec on cpu\n",
      "epoch 188 loss 0.050, 473.2 tokens/sec on cpu\n",
      "epoch 189 loss 0.049, 481.6 tokens/sec on cpu\n",
      "epoch 190 loss 0.046, 437.0 tokens/sec on cpu\n",
      "epoch 191 loss 0.045, 479.6 tokens/sec on cpu\n",
      "epoch 192 loss 0.049, 483.5 tokens/sec on cpu\n",
      "epoch 193 loss 0.045, 503.7 tokens/sec on cpu\n",
      "epoch 194 loss 0.047, 463.8 tokens/sec on cpu\n",
      "epoch 195 loss 0.048, 470.2 tokens/sec on cpu\n",
      "epoch 196 loss 0.046, 456.5 tokens/sec on cpu\n",
      "epoch 197 loss 0.048, 496.6 tokens/sec on cpu\n",
      "epoch 198 loss 0.045, 505.3 tokens/sec on cpu\n",
      "epoch 199 loss 0.048, 470.1 tokens/sec on cpu\n",
      "epoch 200 loss 0.046, 508.5 tokens/sec on cpu\n",
      "epoch 201 loss 0.047, 526.4 tokens/sec on cpu\n",
      "epoch 202 loss 0.047, 438.6 tokens/sec on cpu\n",
      "epoch 203 loss 0.047, 537.0 tokens/sec on cpu\n",
      "epoch 204 loss 0.046, 487.5 tokens/sec on cpu\n",
      "epoch 205 loss 0.049, 463.8 tokens/sec on cpu\n",
      "epoch 206 loss 0.047, 503.8 tokens/sec on cpu\n",
      "epoch 207 loss 0.047, 496.4 tokens/sec on cpu\n",
      "epoch 208 loss 0.045, 466.8 tokens/sec on cpu\n",
      "epoch 209 loss 0.047, 537.9 tokens/sec on cpu\n",
      "epoch 210 loss 0.048, 465.0 tokens/sec on cpu\n",
      "epoch 211 loss 0.048, 514.3 tokens/sec on cpu\n",
      "epoch 212 loss 0.046, 505.8 tokens/sec on cpu\n",
      "epoch 213 loss 0.048, 552.2 tokens/sec on cpu\n",
      "epoch 214 loss 0.047, 531.6 tokens/sec on cpu\n",
      "epoch 215 loss 0.046, 496.0 tokens/sec on cpu\n",
      "epoch 216 loss 0.046, 459.5 tokens/sec on cpu\n",
      "epoch 217 loss 0.046, 465.6 tokens/sec on cpu\n",
      "epoch 218 loss 0.045, 450.3 tokens/sec on cpu\n",
      "epoch 219 loss 0.048, 544.5 tokens/sec on cpu\n",
      "epoch 220 loss 0.047, 523.2 tokens/sec on cpu\n",
      "epoch 221 loss 0.048, 590.6 tokens/sec on cpu\n",
      "epoch 222 loss 0.046, 571.1 tokens/sec on cpu\n",
      "epoch 223 loss 0.044, 571.5 tokens/sec on cpu\n",
      "epoch 224 loss 0.048, 580.7 tokens/sec on cpu\n",
      "epoch 225 loss 0.045, 572.2 tokens/sec on cpu\n",
      "epoch 226 loss 0.043, 522.0 tokens/sec on cpu\n",
      "epoch 227 loss 0.046, 549.8 tokens/sec on cpu\n",
      "epoch 228 loss 0.047, 498.8 tokens/sec on cpu\n",
      "epoch 229 loss 0.048, 516.0 tokens/sec on cpu\n",
      "epoch 230 loss 0.047, 533.1 tokens/sec on cpu\n",
      "epoch 231 loss 0.046, 491.0 tokens/sec on cpu\n",
      "epoch 232 loss 0.043, 451.8 tokens/sec on cpu\n",
      "epoch 233 loss 0.046, 454.0 tokens/sec on cpu\n",
      "epoch 234 loss 0.045, 480.0 tokens/sec on cpu\n",
      "epoch 235 loss 0.043, 480.3 tokens/sec on cpu\n",
      "epoch 236 loss 0.044, 421.5 tokens/sec on cpu\n",
      "epoch 237 loss 0.045, 417.7 tokens/sec on cpu\n",
      "epoch 238 loss 0.043, 573.6 tokens/sec on cpu\n",
      "epoch 239 loss 0.046, 561.3 tokens/sec on cpu\n",
      "epoch 240 loss 0.043, 576.2 tokens/sec on cpu\n",
      "epoch 241 loss 0.044, 492.3 tokens/sec on cpu\n",
      "epoch 242 loss 0.046, 535.1 tokens/sec on cpu\n",
      "epoch 243 loss 0.044, 565.1 tokens/sec on cpu\n",
      "epoch 244 loss 0.044, 560.6 tokens/sec on cpu\n",
      "epoch 245 loss 0.043, 562.7 tokens/sec on cpu\n",
      "epoch 246 loss 0.047, 501.9 tokens/sec on cpu\n",
      "epoch 247 loss 0.042, 501.6 tokens/sec on cpu\n",
      "epoch 248 loss 0.043, 469.3 tokens/sec on cpu\n",
      "epoch 249 loss 0.044, 479.1 tokens/sec on cpu\n",
      "epoch 250 loss 0.046, 521.8 tokens/sec on cpu\n",
      "epoch 251 loss 0.044, 514.8 tokens/sec on cpu\n",
      "epoch 252 loss 0.046, 527.2 tokens/sec on cpu\n",
      "epoch 253 loss 0.044, 531.3 tokens/sec on cpu\n",
      "epoch 254 loss 0.044, 295.4 tokens/sec on cpu\n",
      "epoch 255 loss 0.045, 280.9 tokens/sec on cpu\n",
      "epoch 256 loss 0.045, 287.8 tokens/sec on cpu\n",
      "epoch 257 loss 0.045, 288.4 tokens/sec on cpu\n",
      "epoch 258 loss 0.045, 381.3 tokens/sec on cpu\n",
      "epoch 259 loss 0.042, 362.9 tokens/sec on cpu\n",
      "epoch 260 loss 0.044, 397.3 tokens/sec on cpu\n",
      "epoch 261 loss 0.044, 373.1 tokens/sec on cpu\n",
      "epoch 262 loss 0.044, 485.5 tokens/sec on cpu\n",
      "epoch 263 loss 0.040, 400.7 tokens/sec on cpu\n",
      "epoch 264 loss 0.043, 358.1 tokens/sec on cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\比赛_项目\\NLP_camp\\my_transformer\\train.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%AF%94%E8%B5%9B_%E9%A1%B9%E7%9B%AE/NLP_camp/my_transformer/train.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model \u001b[39m=\u001b[39m MYTransformer(ndim, vocab_src, vocab_tar, drop_v, h, ndim_hidden, num_layers, masked_len)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%AF%94%E8%B5%9B_%E9%A1%B9%E7%9B%AE/NLP_camp/my_transformer/train.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m lr, num_epochs, device \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m, \u001b[39m300\u001b[39m, torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%E6%AF%94%E8%B5%9B_%E9%A1%B9%E7%9B%AE/NLP_camp/my_transformer/train.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m epoches, loss_per_epoch \u001b[39m=\u001b[39m train_seq2seq(model, train_iter, lr, num_epochs, tgt_vocab, device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%AF%94%E8%B5%9B_%E9%A1%B9%E7%9B%AE/NLP_camp/my_transformer/train.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(epoches, loss_per_epoch)\n",
      "\u001b[1;32md:\\比赛_项目\\NLP_camp\\my_transformer\\train.ipynb Cell 5\u001b[0m in \u001b[0;36mtrain_seq2seq\u001b[1;34m(net, data_iter, lr, num_epochs, tgt_vocab, device, from_scrach)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/%E6%AF%94%E8%B5%9B_%E9%A1%B9%E7%9B%AE/NLP_camp/my_transformer/train.ipynb#W4sZmlsZQ%3D%3D?line=193'>194</a>\u001b[0m Y_hat \u001b[39m=\u001b[39m net(X, dec_input)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/%E6%AF%94%E8%B5%9B_%E9%A1%B9%E7%9B%AE/NLP_camp/my_transformer/train.ipynb#W4sZmlsZQ%3D%3D?line=194'>195</a>\u001b[0m l \u001b[39m=\u001b[39m loss(Y_hat, Y, Y_valid_len)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/%E6%AF%94%E8%B5%9B_%E9%A1%B9%E7%9B%AE/NLP_camp/my_transformer/train.ipynb#W4sZmlsZQ%3D%3D?line=195'>196</a>\u001b[0m l\u001b[39m.\u001b[39;49msum()\u001b[39m.\u001b[39;49mbackward()      \u001b[39m# 损失函数的标量进行“反向传播”\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/%E6%AF%94%E8%B5%9B_%E9%A1%B9%E7%9B%AE/NLP_camp/my_transformer/train.ipynb#W4sZmlsZQ%3D%3D?line=196'>197</a>\u001b[0m grad_clipping(net, \u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/%E6%AF%94%E8%B5%9B_%E9%A1%B9%E7%9B%AE/NLP_camp/my_transformer/train.ipynb#W4sZmlsZQ%3D%3D?line=198'>199</a>\u001b[0m num_tokens \u001b[39m=\u001b[39m Y_valid_len\u001b[39m.\u001b[39msum()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_iter, src_vocab, tgt_vocab = load_data(batch_size=2, num_steps=10, data_path=r'data\\fra-eng\\fra.txt')\n",
    "ndim = 32\n",
    "vocab_src = len(src_vocab)\n",
    "vocab_tar = len(tgt_vocab)\n",
    "drop_v = 0.1 \n",
    "h =4\n",
    "ndim_hidden = 32 \n",
    "num_layers = 2 \n",
    "masked_len = max(vocab_src, vocab_tar)\n",
    "model = MYTransformer(ndim, vocab_src, vocab_tar, drop_v, h, ndim_hidden, num_layers, masked_len)\n",
    "lr, num_epochs, device = 0.001, 300, torch.device('cpu')\n",
    "epoches, loss_per_epoch = train_seq2seq(model, train_iter, lr, num_epochs, tgt_vocab, device)\n",
    "plt.plot(epoches, loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0437)\n"
     ]
    }
   ],
   "source": [
    "download_model = torch.load(r'model.pth')\n",
    "print(min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.109, 500.0 tokens/sec on cpu\n",
      "epoch 1 loss 0.080, 484.7 tokens/sec on cpu\n",
      "epoch 2 loss 0.070, 482.8 tokens/sec on cpu\n",
      "epoch 3 loss 0.063, 471.6 tokens/sec on cpu\n",
      "epoch 4 loss 0.063, 485.8 tokens/sec on cpu\n",
      "epoch 5 loss 0.058, 487.1 tokens/sec on cpu\n",
      "epoch 6 loss 0.057, 484.9 tokens/sec on cpu\n",
      "epoch 7 loss 0.056, 489.2 tokens/sec on cpu\n",
      "epoch 8 loss 0.057, 488.6 tokens/sec on cpu\n",
      "epoch 9 loss 0.056, 494.2 tokens/sec on cpu\n",
      "epoch 10 loss 0.053, 485.9 tokens/sec on cpu\n",
      "epoch 11 loss 0.054, 468.2 tokens/sec on cpu\n",
      "epoch 12 loss 0.051, 482.2 tokens/sec on cpu\n",
      "epoch 13 loss 0.054, 470.5 tokens/sec on cpu\n",
      "epoch 14 loss 0.052, 473.8 tokens/sec on cpu\n",
      "epoch 15 loss 0.050, 479.8 tokens/sec on cpu\n",
      "epoch 16 loss 0.052, 484.8 tokens/sec on cpu\n",
      "epoch 17 loss 0.052, 481.8 tokens/sec on cpu\n",
      "epoch 18 loss 0.053, 483.5 tokens/sec on cpu\n",
      "epoch 19 loss 0.051, 469.9 tokens/sec on cpu\n",
      "epoch 20 loss 0.054, 480.5 tokens/sec on cpu\n",
      "epoch 21 loss 0.052, 483.1 tokens/sec on cpu\n",
      "epoch 22 loss 0.050, 484.1 tokens/sec on cpu\n",
      "epoch 23 loss 0.051, 471.1 tokens/sec on cpu\n",
      "epoch 24 loss 0.049, 479.0 tokens/sec on cpu\n",
      "epoch 25 loss 0.050, 486.7 tokens/sec on cpu\n",
      "epoch 26 loss 0.049, 487.5 tokens/sec on cpu\n",
      "epoch 27 loss 0.047, 483.0 tokens/sec on cpu\n",
      "epoch 28 loss 0.049, 484.0 tokens/sec on cpu\n",
      "epoch 29 loss 0.049, 494.0 tokens/sec on cpu\n",
      "epoch 30 loss 0.047, 492.8 tokens/sec on cpu\n",
      "epoch 31 loss 0.049, 465.3 tokens/sec on cpu\n",
      "epoch 32 loss 0.049, 490.4 tokens/sec on cpu\n",
      "epoch 33 loss 0.050, 487.0 tokens/sec on cpu\n",
      "epoch 34 loss 0.047, 489.5 tokens/sec on cpu\n",
      "epoch 35 loss 0.046, 483.3 tokens/sec on cpu\n",
      "epoch 36 loss 0.047, 480.1 tokens/sec on cpu\n",
      "epoch 37 loss 0.049, 480.0 tokens/sec on cpu\n",
      "epoch 38 loss 0.045, 454.8 tokens/sec on cpu\n",
      "epoch 39 loss 0.048, 450.5 tokens/sec on cpu\n",
      "epoch 40 loss 0.047, 474.4 tokens/sec on cpu\n",
      "epoch 41 loss 0.050, 481.4 tokens/sec on cpu\n",
      "epoch 42 loss 0.046, 477.4 tokens/sec on cpu\n",
      "epoch 43 loss 0.048, 486.7 tokens/sec on cpu\n",
      "epoch 44 loss 0.045, 466.6 tokens/sec on cpu\n",
      "epoch 45 loss 0.048, 483.6 tokens/sec on cpu\n",
      "epoch 46 loss 0.045, 485.8 tokens/sec on cpu\n",
      "epoch 47 loss 0.046, 486.4 tokens/sec on cpu\n",
      "epoch 48 loss 0.048, 484.3 tokens/sec on cpu\n",
      "epoch 49 loss 0.048, 482.0 tokens/sec on cpu\n",
      "epoch 50 loss 0.046, 494.3 tokens/sec on cpu\n",
      "epoch 51 loss 0.043, 496.4 tokens/sec on cpu\n",
      "epoch 52 loss 0.046, 492.2 tokens/sec on cpu\n",
      "epoch 53 loss 0.046, 488.3 tokens/sec on cpu\n",
      "epoch 54 loss 0.047, 486.1 tokens/sec on cpu\n",
      "epoch 55 loss 0.047, 493.7 tokens/sec on cpu\n",
      "epoch 56 loss 0.046, 471.6 tokens/sec on cpu\n",
      "epoch 57 loss 0.045, 481.6 tokens/sec on cpu\n",
      "epoch 58 loss 0.046, 490.7 tokens/sec on cpu\n",
      "epoch 59 loss 0.048, 484.4 tokens/sec on cpu\n",
      "epoch 60 loss 0.045, 488.5 tokens/sec on cpu\n",
      "epoch 61 loss 0.044, 466.9 tokens/sec on cpu\n",
      "epoch 62 loss 0.046, 484.0 tokens/sec on cpu\n",
      "epoch 63 loss 0.043, 478.5 tokens/sec on cpu\n",
      "epoch 64 loss 0.045, 477.1 tokens/sec on cpu\n",
      "epoch 65 loss 0.044, 471.7 tokens/sec on cpu\n",
      "epoch 66 loss 0.043, 490.2 tokens/sec on cpu\n",
      "epoch 67 loss 0.044, 482.3 tokens/sec on cpu\n",
      "epoch 68 loss 0.042, 477.7 tokens/sec on cpu\n",
      "epoch 69 loss 0.045, 480.5 tokens/sec on cpu\n",
      "epoch 70 loss 0.042, 491.2 tokens/sec on cpu\n",
      "epoch 71 loss 0.045, 484.0 tokens/sec on cpu\n",
      "epoch 72 loss 0.043, 470.0 tokens/sec on cpu\n",
      "epoch 73 loss 0.043, 478.8 tokens/sec on cpu\n",
      "epoch 74 loss 0.044, 485.2 tokens/sec on cpu\n",
      "epoch 75 loss 0.046, 485.1 tokens/sec on cpu\n",
      "epoch 76 loss 0.042, 490.9 tokens/sec on cpu\n",
      "epoch 77 loss 0.043, 483.2 tokens/sec on cpu\n",
      "epoch 78 loss 0.045, 489.4 tokens/sec on cpu\n",
      "epoch 79 loss 0.043, 486.6 tokens/sec on cpu\n",
      "epoch 80 loss 0.044, 480.2 tokens/sec on cpu\n",
      "epoch 81 loss 0.042, 489.9 tokens/sec on cpu\n",
      "epoch 82 loss 0.044, 490.0 tokens/sec on cpu\n",
      "epoch 83 loss 0.044, 496.7 tokens/sec on cpu\n",
      "epoch 84 loss 0.042, 484.1 tokens/sec on cpu\n",
      "epoch 85 loss 0.045, 489.9 tokens/sec on cpu\n",
      "epoch 86 loss 0.042, 448.9 tokens/sec on cpu\n",
      "epoch 87 loss 0.046, 469.4 tokens/sec on cpu\n",
      "epoch 88 loss 0.044, 464.1 tokens/sec on cpu\n",
      "epoch 89 loss 0.046, 477.2 tokens/sec on cpu\n",
      "epoch 90 loss 0.043, 482.0 tokens/sec on cpu\n",
      "epoch 91 loss 0.045, 478.8 tokens/sec on cpu\n",
      "epoch 92 loss 0.044, 486.5 tokens/sec on cpu\n",
      "epoch 93 loss 0.043, 480.9 tokens/sec on cpu\n",
      "epoch 94 loss 0.041, 484.6 tokens/sec on cpu\n",
      "epoch 95 loss 0.043, 462.0 tokens/sec on cpu\n",
      "epoch 96 loss 0.046, 484.0 tokens/sec on cpu\n",
      "epoch 97 loss 0.044, 483.7 tokens/sec on cpu\n",
      "epoch 98 loss 0.044, 475.6 tokens/sec on cpu\n",
      "epoch 99 loss 0.044, 478.3 tokens/sec on cpu\n",
      "epoch 100 loss 0.045, 487.0 tokens/sec on cpu\n",
      "epoch 101 loss 0.042, 484.4 tokens/sec on cpu\n",
      "epoch 102 loss 0.044, 492.1 tokens/sec on cpu\n",
      "epoch 103 loss 0.042, 491.5 tokens/sec on cpu\n",
      "epoch 104 loss 0.043, 482.9 tokens/sec on cpu\n",
      "epoch 105 loss 0.042, 482.9 tokens/sec on cpu\n",
      "epoch 106 loss 0.043, 483.8 tokens/sec on cpu\n",
      "epoch 107 loss 0.042, 483.4 tokens/sec on cpu\n",
      "epoch 108 loss 0.044, 491.1 tokens/sec on cpu\n",
      "epoch 109 loss 0.044, 490.9 tokens/sec on cpu\n",
      "epoch 110 loss 0.042, 491.2 tokens/sec on cpu\n",
      "epoch 111 loss 0.042, 483.9 tokens/sec on cpu\n",
      "epoch 112 loss 0.042, 481.1 tokens/sec on cpu\n",
      "epoch 113 loss 0.042, 485.1 tokens/sec on cpu\n",
      "epoch 114 loss 0.041, 482.9 tokens/sec on cpu\n",
      "epoch 115 loss 0.042, 476.5 tokens/sec on cpu\n",
      "epoch 116 loss 0.044, 459.1 tokens/sec on cpu\n",
      "epoch 117 loss 0.042, 486.4 tokens/sec on cpu\n",
      "epoch 118 loss 0.043, 478.3 tokens/sec on cpu\n",
      "epoch 119 loss 0.042, 483.7 tokens/sec on cpu\n",
      "epoch 120 loss 0.040, 466.6 tokens/sec on cpu\n",
      "epoch 121 loss 0.043, 484.0 tokens/sec on cpu\n",
      "epoch 122 loss 0.041, 475.3 tokens/sec on cpu\n",
      "epoch 123 loss 0.044, 485.7 tokens/sec on cpu\n",
      "epoch 124 loss 0.040, 484.4 tokens/sec on cpu\n",
      "epoch 125 loss 0.041, 485.0 tokens/sec on cpu\n",
      "epoch 126 loss 0.042, 487.4 tokens/sec on cpu\n",
      "epoch 127 loss 0.041, 494.1 tokens/sec on cpu\n",
      "epoch 128 loss 0.045, 484.2 tokens/sec on cpu\n",
      "epoch 129 loss 0.041, 481.0 tokens/sec on cpu\n",
      "epoch 130 loss 0.041, 498.5 tokens/sec on cpu\n",
      "epoch 131 loss 0.041, 494.0 tokens/sec on cpu\n",
      "epoch 132 loss 0.041, 494.1 tokens/sec on cpu\n",
      "epoch 133 loss 0.044, 487.7 tokens/sec on cpu\n",
      "epoch 134 loss 0.042, 486.3 tokens/sec on cpu\n",
      "epoch 135 loss 0.040, 494.5 tokens/sec on cpu\n",
      "epoch 136 loss 0.041, 480.9 tokens/sec on cpu\n",
      "epoch 137 loss 0.045, 474.5 tokens/sec on cpu\n",
      "epoch 138 loss 0.042, 481.6 tokens/sec on cpu\n",
      "epoch 139 loss 0.042, 481.3 tokens/sec on cpu\n",
      "epoch 140 loss 0.042, 478.6 tokens/sec on cpu\n",
      "epoch 141 loss 0.039, 465.4 tokens/sec on cpu\n",
      "epoch 142 loss 0.042, 479.4 tokens/sec on cpu\n",
      "epoch 143 loss 0.041, 489.1 tokens/sec on cpu\n",
      "epoch 144 loss 0.040, 490.9 tokens/sec on cpu\n",
      "epoch 145 loss 0.042, 456.3 tokens/sec on cpu\n",
      "epoch 146 loss 0.043, 489.8 tokens/sec on cpu\n",
      "epoch 147 loss 0.042, 483.2 tokens/sec on cpu\n",
      "epoch 148 loss 0.040, 487.0 tokens/sec on cpu\n",
      "epoch 149 loss 0.039, 486.4 tokens/sec on cpu\n",
      "epoch 150 loss 0.041, 486.7 tokens/sec on cpu\n",
      "epoch 151 loss 0.042, 485.3 tokens/sec on cpu\n",
      "epoch 152 loss 0.042, 480.2 tokens/sec on cpu\n",
      "epoch 153 loss 0.043, 490.9 tokens/sec on cpu\n",
      "epoch 154 loss 0.042, 491.7 tokens/sec on cpu\n",
      "epoch 155 loss 0.042, 491.2 tokens/sec on cpu\n",
      "epoch 156 loss 0.040, 490.6 tokens/sec on cpu\n",
      "epoch 157 loss 0.040, 490.5 tokens/sec on cpu\n",
      "epoch 158 loss 0.040, 488.8 tokens/sec on cpu\n",
      "epoch 159 loss 0.040, 499.5 tokens/sec on cpu\n",
      "epoch 160 loss 0.039, 494.5 tokens/sec on cpu\n",
      "epoch 161 loss 0.040, 491.1 tokens/sec on cpu\n",
      "epoch 162 loss 0.041, 492.2 tokens/sec on cpu\n",
      "epoch 163 loss 0.041, 478.5 tokens/sec on cpu\n",
      "epoch 164 loss 0.040, 485.1 tokens/sec on cpu\n",
      "epoch 165 loss 0.040, 481.3 tokens/sec on cpu\n",
      "epoch 166 loss 0.039, 471.6 tokens/sec on cpu\n",
      "epoch 167 loss 0.041, 474.9 tokens/sec on cpu\n",
      "epoch 168 loss 0.041, 484.3 tokens/sec on cpu\n",
      "epoch 169 loss 0.042, 485.1 tokens/sec on cpu\n",
      "epoch 170 loss 0.041, 484.0 tokens/sec on cpu\n",
      "epoch 171 loss 0.041, 486.0 tokens/sec on cpu\n",
      "epoch 172 loss 0.041, 485.2 tokens/sec on cpu\n",
      "epoch 173 loss 0.043, 482.9 tokens/sec on cpu\n",
      "epoch 174 loss 0.040, 483.4 tokens/sec on cpu\n",
      "epoch 175 loss 0.040, 488.1 tokens/sec on cpu\n",
      "epoch 176 loss 0.042, 477.0 tokens/sec on cpu\n",
      "epoch 177 loss 0.040, 486.1 tokens/sec on cpu\n",
      "epoch 178 loss 0.040, 494.2 tokens/sec on cpu\n",
      "epoch 179 loss 0.041, 490.6 tokens/sec on cpu\n",
      "epoch 180 loss 0.042, 480.1 tokens/sec on cpu\n",
      "epoch 181 loss 0.042, 487.6 tokens/sec on cpu\n",
      "epoch 182 loss 0.043, 487.3 tokens/sec on cpu\n",
      "epoch 183 loss 0.040, 487.9 tokens/sec on cpu\n",
      "epoch 184 loss 0.039, 513.1 tokens/sec on cpu\n",
      "epoch 185 loss 0.041, 0.6 tokens/sec on cpu\n",
      "epoch 186 loss 0.039, 310.7 tokens/sec on cpu\n",
      "epoch 187 loss 0.038, 463.1 tokens/sec on cpu\n",
      "epoch 188 loss 0.041, 477.7 tokens/sec on cpu\n",
      "epoch 189 loss 0.039, 475.4 tokens/sec on cpu\n",
      "epoch 190 loss 0.042, 481.3 tokens/sec on cpu\n",
      "epoch 191 loss 0.039, 448.2 tokens/sec on cpu\n",
      "epoch 192 loss 0.043, 471.1 tokens/sec on cpu\n",
      "epoch 193 loss 0.040, 488.4 tokens/sec on cpu\n",
      "epoch 194 loss 0.041, 420.0 tokens/sec on cpu\n",
      "epoch 195 loss 0.040, 462.5 tokens/sec on cpu\n",
      "epoch 196 loss 0.039, 444.3 tokens/sec on cpu\n",
      "epoch 197 loss 0.041, 485.7 tokens/sec on cpu\n",
      "epoch 198 loss 0.040, 481.5 tokens/sec on cpu\n",
      "epoch 199 loss 0.038, 482.3 tokens/sec on cpu\n",
      "epoch 200 loss 0.040, 489.9 tokens/sec on cpu\n",
      "epoch 201 loss 0.040, 484.9 tokens/sec on cpu\n",
      "epoch 202 loss 0.040, 476.0 tokens/sec on cpu\n",
      "epoch 203 loss 0.042, 486.3 tokens/sec on cpu\n",
      "epoch 204 loss 0.043, 494.0 tokens/sec on cpu\n",
      "epoch 205 loss 0.037, 487.2 tokens/sec on cpu\n",
      "epoch 206 loss 0.042, 480.4 tokens/sec on cpu\n",
      "epoch 207 loss 0.040, 490.0 tokens/sec on cpu\n",
      "epoch 208 loss 0.039, 494.0 tokens/sec on cpu\n",
      "epoch 209 loss 0.039, 495.8 tokens/sec on cpu\n",
      "epoch 210 loss 0.042, 495.5 tokens/sec on cpu\n",
      "epoch 211 loss 0.037, 497.0 tokens/sec on cpu\n",
      "epoch 212 loss 0.039, 492.8 tokens/sec on cpu\n",
      "epoch 213 loss 0.040, 495.1 tokens/sec on cpu\n",
      "epoch 214 loss 0.043, 492.0 tokens/sec on cpu\n",
      "epoch 215 loss 0.042, 493.0 tokens/sec on cpu\n",
      "epoch 216 loss 0.040, 497.3 tokens/sec on cpu\n",
      "epoch 217 loss 0.040, 486.7 tokens/sec on cpu\n",
      "epoch 218 loss 0.040, 409.9 tokens/sec on cpu\n",
      "epoch 219 loss 0.039, 448.7 tokens/sec on cpu\n",
      "epoch 220 loss 0.038, 483.3 tokens/sec on cpu\n",
      "epoch 221 loss 0.040, 488.5 tokens/sec on cpu\n",
      "epoch 222 loss 0.039, 423.9 tokens/sec on cpu\n",
      "epoch 223 loss 0.038, 451.4 tokens/sec on cpu\n",
      "epoch 224 loss 0.040, 452.8 tokens/sec on cpu\n",
      "epoch 225 loss 0.040, 472.9 tokens/sec on cpu\n",
      "epoch 226 loss 0.040, 494.0 tokens/sec on cpu\n",
      "epoch 227 loss 0.039, 482.3 tokens/sec on cpu\n",
      "epoch 228 loss 0.040, 446.2 tokens/sec on cpu\n",
      "epoch 229 loss 0.041, 445.6 tokens/sec on cpu\n",
      "epoch 230 loss 0.040, 483.3 tokens/sec on cpu\n",
      "epoch 231 loss 0.038, 486.7 tokens/sec on cpu\n",
      "epoch 232 loss 0.039, 486.0 tokens/sec on cpu\n",
      "epoch 233 loss 0.039, 491.4 tokens/sec on cpu\n",
      "epoch 234 loss 0.039, 491.3 tokens/sec on cpu\n",
      "epoch 235 loss 0.041, 485.5 tokens/sec on cpu\n",
      "epoch 236 loss 0.041, 488.3 tokens/sec on cpu\n",
      "epoch 237 loss 0.038, 486.1 tokens/sec on cpu\n",
      "epoch 238 loss 0.041, 474.7 tokens/sec on cpu\n",
      "epoch 239 loss 0.039, 485.8 tokens/sec on cpu\n",
      "epoch 240 loss 0.041, 473.1 tokens/sec on cpu\n",
      "epoch 241 loss 0.039, 474.8 tokens/sec on cpu\n",
      "epoch 242 loss 0.043, 485.6 tokens/sec on cpu\n",
      "epoch 243 loss 0.040, 484.7 tokens/sec on cpu\n",
      "epoch 244 loss 0.037, 487.3 tokens/sec on cpu\n",
      "epoch 245 loss 0.038, 486.3 tokens/sec on cpu\n",
      "epoch 246 loss 0.040, 489.4 tokens/sec on cpu\n",
      "epoch 247 loss 0.038, 490.4 tokens/sec on cpu\n",
      "epoch 248 loss 0.039, 493.2 tokens/sec on cpu\n",
      "epoch 249 loss 0.039, 483.2 tokens/sec on cpu\n",
      "epoch 250 loss 0.038, 481.5 tokens/sec on cpu\n",
      "epoch 251 loss 0.038, 481.1 tokens/sec on cpu\n",
      "epoch 252 loss 0.041, 486.5 tokens/sec on cpu\n",
      "epoch 253 loss 0.041, 472.5 tokens/sec on cpu\n",
      "epoch 254 loss 0.037, 469.4 tokens/sec on cpu\n",
      "epoch 255 loss 0.040, 470.5 tokens/sec on cpu\n",
      "epoch 256 loss 0.037, 482.5 tokens/sec on cpu\n",
      "epoch 257 loss 0.038, 491.3 tokens/sec on cpu\n",
      "epoch 258 loss 0.039, 485.1 tokens/sec on cpu\n",
      "epoch 259 loss 0.039, 493.9 tokens/sec on cpu\n",
      "epoch 260 loss 0.039, 490.9 tokens/sec on cpu\n",
      "epoch 261 loss 0.039, 491.4 tokens/sec on cpu\n",
      "epoch 262 loss 0.038, 491.8 tokens/sec on cpu\n",
      "epoch 263 loss 0.039, 485.7 tokens/sec on cpu\n",
      "epoch 264 loss 0.038, 489.9 tokens/sec on cpu\n",
      "epoch 265 loss 0.037, 493.2 tokens/sec on cpu\n",
      "epoch 266 loss 0.038, 491.9 tokens/sec on cpu\n",
      "epoch 267 loss 0.039, 495.0 tokens/sec on cpu\n",
      "epoch 268 loss 0.040, 499.3 tokens/sec on cpu\n",
      "epoch 269 loss 0.038, 494.4 tokens/sec on cpu\n",
      "epoch 270 loss 0.037, 486.8 tokens/sec on cpu\n",
      "epoch 271 loss 0.037, 492.4 tokens/sec on cpu\n",
      "epoch 272 loss 0.039, 488.6 tokens/sec on cpu\n",
      "epoch 273 loss 0.038, 490.5 tokens/sec on cpu\n",
      "epoch 274 loss 0.038, 487.5 tokens/sec on cpu\n",
      "epoch 275 loss 0.039, 483.2 tokens/sec on cpu\n",
      "epoch 276 loss 0.040, 484.9 tokens/sec on cpu\n",
      "epoch 277 loss 0.039, 487.3 tokens/sec on cpu\n",
      "epoch 278 loss 0.037, 474.2 tokens/sec on cpu\n",
      "epoch 279 loss 0.041, 441.5 tokens/sec on cpu\n",
      "epoch 280 loss 0.038, 438.9 tokens/sec on cpu\n",
      "epoch 281 loss 0.038, 421.4 tokens/sec on cpu\n",
      "epoch 282 loss 0.038, 438.3 tokens/sec on cpu\n",
      "epoch 283 loss 0.041, 438.9 tokens/sec on cpu\n",
      "epoch 284 loss 0.038, 441.1 tokens/sec on cpu\n",
      "epoch 285 loss 0.039, 390.2 tokens/sec on cpu\n",
      "epoch 286 loss 0.037, 402.7 tokens/sec on cpu\n",
      "epoch 287 loss 0.038, 427.9 tokens/sec on cpu\n",
      "epoch 288 loss 0.038, 431.8 tokens/sec on cpu\n",
      "epoch 289 loss 0.039, 432.4 tokens/sec on cpu\n",
      "epoch 290 loss 0.042, 437.6 tokens/sec on cpu\n",
      "epoch 291 loss 0.039, 379.4 tokens/sec on cpu\n",
      "epoch 292 loss 0.040, 404.7 tokens/sec on cpu\n",
      "epoch 293 loss 0.038, 408.7 tokens/sec on cpu\n",
      "epoch 294 loss 0.038, 414.7 tokens/sec on cpu\n",
      "epoch 295 loss 0.039, 431.7 tokens/sec on cpu\n",
      "epoch 296 loss 0.038, 412.5 tokens/sec on cpu\n",
      "epoch 297 loss 0.037, 402.8 tokens/sec on cpu\n",
      "epoch 298 loss 0.038, 417.9 tokens/sec on cpu\n",
      "epoch 299 loss 0.040, 420.4 tokens/sec on cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2972c4e6950>]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "lr, num_epochs, device = 0.001, 300, torch.device('cpu')\n",
    "epoches, loss_per_epoch = train_seq2seq(download_model, train_iter, lr, num_epochs, tgt_vocab, device, False)\n",
    "plt.plot(epoches, loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . => nous sois gagné ? pas, \n",
      "i lost . => la <bos> la <bos> la <bos> la <bos>, \n",
      "he's calm . => la <bos> la <bos> la <bos> la <bos>, \n",
      "i'm home . => un j'ai il gagné ? pas, \n"
     ]
    }
   ],
   "source": [
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "num_steps = 8\n",
    "\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation = predict_seq2seq(\n",
    "        download_model, eng, src_vocab, tgt_vocab, num_steps, device, True)\n",
    "    print(f'{eng} => {translation}, ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
